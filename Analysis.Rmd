---
title: "Untitled"
author: "Lux"
date: "2025-05-27"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)



```


```{r echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
library(ggplot2)
library(dplyr)
library(readr)
library(readxl)
library(tidyr)
library(data.table)
library(openxlsx)
library(tidyverse)
library(lubridate)
library(hms)
library(readxl)
library(xlsx)
library(here)
library(kableExtra)
library(DT)
library(purrr)
library(data.table)
library(tidytext)
library(dplyr)
library(lubridate)
library(anytime)
library(grid)
library(wordcloud)
library(reshape2)
library(ggraph)
library(widyr)
library(topicmodels)
library(ggthemes)
library(xlsx)
library(knitr)
library(kableExtra)
library(stopwords)
library(igraph)
library(ggraph)

```



```{r echo=FALSE, eval=FALSE}

data_0 <- read_excel("./data.xlsx", sheet = 1, col_names = TRUE)
data_1 <- read_excel("./filtered_with_keywords_22.xlsx", sheet = 1, col_names = TRUE)
data_2 <- read_excel("./filtered_with_keywords_23.xlsx", sheet = 1, col_names = TRUE)
data_3 <- bind_rows(data_1, data_2)
data <- merge(data_0, data_3, by = "URL", all.x = TRUE)
write.xlsx(data, file = "dta.xlsx", row.names = FALSE, col.names = TRUE)

```


```{r ch2, echo=F, eval=T, message=F , warning= FALSE, message=F}
source(here("Text_analysis", "stemmer.R"))
source(here("Text_analysis", "text_analysis.R"))
source(here("Text_analysis", "write_tokens.R"))
```


```{r echo=FALSE, eval=TRUE}
data <- read_excel("./dta.xlsx", sheet = 1, col_names = TRUE)

```

```{r echo=FALSE, eval=TRUE}
names(data)
```

```{r echo=FALSE, eval=TRUE}
summary(data)
```



```{r prepare-data, echo=FALSE, eval=TRUE}
# Assume your data frame is named `df`. If not, replace `df` everywhere below
# Convert DATE to Date class, TIME to hms, extract HOUR, and ensure OKVIR is a factor
df <- data %>%
  mutate(
    # Parse DATE and TIME
    DATE  = as.Date(DATE),
    TIME  = as_hms(TIME),
    
    # Extract hour
    HOUR  = hour(TIME),
    
    # Convert OKVIR to factor
    OKVIR = factor(OKVIR),
    
    # Extract month name (full name) as an ordered factor (Jan → December)
    MONTH = month(DATE, label = TRUE, abbr = FALSE),
    
    # Rename FROM_SITE → MEDIJ
    MEDIJ = FROM_SITE
  )
```


# Text analysis (TITLE)

```{r echo=F, eval=T, message=F , warning= FALSE}
# read in lexicons
CroSentilex_n <- read.delim("C:/Users/Lukas/Dropbox/Mislav@Luka/crosentilex-negatives.txt",
                                   header = FALSE,
                                   sep = " ",
                                   stringsAsFactors = FALSE,
                                   fileEncoding = "UTF-8")  %>%
                   rename(word = "V1", sentiment = "V2" ) %>%
                   mutate(brija = "NEG")
 
CroSentilex_p  <- read.delim("C:/Users/Lukas/Dropbox/Mislav@Luka/crosentilex-positives.txt",
                                   header = FALSE,
                                   sep = " ",
                                   stringsAsFactors = FALSE,
                                   fileEncoding = "UTF-8") %>%
                    rename(word = "V1", sentiment = "V2" ) %>%
                    mutate(brija = "POZ")
 
Crosentilex_sve <- rbind(setDT(CroSentilex_n), setDT(CroSentilex_p))
# check lexicon data 
#head(sample_n(Crosentilex_sve,1000),15)

 
CroSentilex_Gold  <- read.delim2("C:/Users/Lukas/Dropbox/Mislav@Luka/gs-sentiment-annotations.txt",
                                 header = FALSE,
                                 sep = " ",
                                 stringsAsFactors = FALSE) %>%
                    rename(word = "V1", sentiment = "V2" ) 
 Encoding(CroSentilex_Gold$word) <- "UTF-8"
 CroSentilex_Gold[1,1] <- "dati"
 CroSentilex_Gold$sentiment <- str_replace(CroSentilex_Gold$sentiment , "-", "1")
 CroSentilex_Gold$sentiment <- str_replace(CroSentilex_Gold$sentiment , "\\+", "2")
 CroSentilex_Gold$sentiment <- as.numeric(unlist(CroSentilex_Gold$sentiment))
# check lexicon data 
#head(sample_n(CroSentilex_Gold,100),15)

 
LilaHR  <- read_excel("C:/Users/Lukas/Dropbox/Mislav@Luka/lilaHR_clean.xlsx", sheet = "Sheet1") %>% select (-"...1")
LilaHR_long <- read_excel("C:/Users/Lukas/Dropbox/Mislav@Luka/lilaHR_clean_long.xlsx", sheet = "Sheet1") %>% select (-"...1") 



# Print the long format data
#print(data_long)

#proba <- read.csv2("C:/Users/Lukas/Dropbox/Mislav@Luka/lilaHRcsv.csv", encoding = "UTF-8")
#df <- separate_rows(LilaHR, HR, sep = ", ") 
# 
# zero_rows_count <- sum(apply(df[-1], 1, function(row) all(row == 0)))
# print(zero_rows_count)
# 
# filtered_df <- df %>% 
#   filter(!apply(.[,-1], 1, function(row) all(row == 0)))
#  
# write.xlsx(filtered_df, "C:/Users/Lukas/Dropbox/Mislav@Luka/lilaHR_.xlsx" )

  
# create stop words
stopwords_cro <- get_stopwords(language = "hr", source = "stopwords-iso")
# check stopwords data
#head(sample_n(stopwords_cro,100),15)
# extend stop words
my_stop_words <- tibble(
  word = c(
    "jedan","mjera", "može", "možete", "mogu", "kad", "sada", "treba", "ima", "osoba",
    "e","prvi", "dva","dvije","drugi",
    "tri","treći","pet","kod",
    "ove","ova",  "ovo","bez", "kod",
    "evo","oko",  "om", "ek",
    "mil","tko","šest", "sedam",
    "osam",   "čim", "zbog",
    "prema", "dok","zato", "koji", 
    "im", "čak","među", "tek",
    "koliko", "tko","kod","poput", 
    "baš", "dakle", "osim", "svih", 
    "svoju", "odnosno", "gdje",
    "kojoj", "ovi", "toga",
     "ubera", "vozača", "hrvatskoj", "usluge", "godine", "više", "taksi", "taxi", "taksija", "taksija", "kaže", "rekao", "19"," aee", "ae","bit.ly", "https", "one", "the"
  ),
  lexicon = "lux"
)

# full set with diacritics
cro_sw_full_d <- tibble(word = c("a","ako","ali","baš","bez","bi","bih","bila","bili","bilo","bio","bismo","bit","biti","bolje","bude","čak","čega","čemu","često","četiri","čime","čini","će","ćemo","ćete","ću","da","dakle","dalje","dan","dana","dana","danas","dio","do","dobro","dok","dosta","dva","dvije","eto","evo","ga","gdje","god","godina","godine","gotovo","grada","i","iako","ići","ih","ili","im","ima","imaju","imali","imam","imao","imati","inače","ipak","isto","iz","iza","između","ja","jako","je","jedan","jedna","jednog","jednom","jednostavno","jednu","jer","joj","još","ju","ka","kad","kada","kaj","kako","kao","kaže","kod","koja","koje","kojeg","kojeg","kojem","koji","kojih","kojim","kojima","kojoj","kojom","koju","koliko","kraju","kroz","li","malo","manje","me","među","međutim","mene","meni","mi","milijuna","mislim","mjesto","mnogo","mogao","mogli","mogu","moj","mora","možda","može","možemo","možete","mu","na","način","nad","naime","nakon","nam","naravno","nas","ne","neće","nego","neka","neke","neki","nekog","nekoliko","neku","nema","nešto","netko","ni","nije","nikad","nisam","nisu","ništa","niti","no","njih","o","od","odmah","odnosno","oko","on","ona","onda","oni","onih","ono","opet","osim","ova","ovaj","ovdje","ove","ovim","ovo","ovog","ovom","ovu","pa","pak","par","po","pod","poput","posto","postoji","pred","preko","prema","pri","prije","protiv","prvi","puno","put","radi","reći","s","sa","sad","sada","sam","samo","sati","se","sebe","si","smo","ste","stoga","strane","su","svaki","sve","svi","svih","svoj","svoje","svoju","što","ta","tada","taj","tako","također","tamo","te","tek","teško","ti","tih","tijekom","time","tko","to","tog","toga","toj","toliko","tom","tome","treba","tu","u","uopće","upravo","uvijek","uz","vam","vas","već","vi","više","vrijeme","vrlo","za","zapravo","zar","zato","zbog","zna","znači"),
                        lexicon = "boras")


stop_corpus <- my_stop_words %>%
  bind_rows(stopwords_cro)


stop_corpus <- stop_corpus %>%
  bind_rows(cro_sw_full_d)

# check stopwords data
#head(sample_n(stop_corpus,100),15)
```

```{r echo=F, eval=T, message=F , warning= FALSE}
# dim before tokenize
#dim(dta)

# tokenize
df %>% 
  unnest_tokens(word, TITLE) -> n_token

# dim after tokenize
#dim(n_token)

# check
# fb_token %>% 
#   select(FROM, word, MENTION_SNIPPET ) %>%
#     sample_n(.,100)

# remove stop words, numbers, single letters
n_token %>% 
  anti_join(stop_corpus, by = "word") %>%
  mutate(word = gsub("\\d+", NA, word)) %>%
  mutate(word = gsub("^[a-zA-Z]$", NA, word)) -> n_tokenTidy
# remove NA
n_tokenTidy %>%
  filter(!is.na(word)) -> n_tokenTidy

# check
# fb_tokenTidy  %>% 
#   select(FROM, word, MENTION_SNIPPET ) %>%
#   sample_n(.,100)

# dim after clean
#dim(n_tokenTidy)

```


## STEMMING

```{r}
# Apply stemming to the "word" column
tidy_text_ <- n_tokenTidy %>%
  mutate(
    stemmed_word = sapply(word, function(w) {
      stemmed = write_tokens(w)  # Apply your write_tokens function
      # Process the result similar to your generalno example
      stemmed = sapply(strsplit(stemmed, "\t"), `[`, 2)  # Extract the second element
      stemmed
    })
  )

# Enframe the results if necessary
tidy_text_ <- tidy_text_ %>%
  rename(original_word = word) %>%
  select(URL, original_word, stemmed_word)
```


```{r}
process_in_batches <- function(data, batch_size = 1000) {
  total_rows <- nrow(data)
  num_batches <- ceiling(total_rows / batch_size)
  
  # Initialize a new column to store stemmed words
  data$stemmed_word <- NA
  
  # Initialize variables to track time
  start_time <- Sys.time()
  batch_times <- numeric(num_batches)  # To store time taken for each batch
  
  for (i in seq_len(num_batches)) {
    # Calculate the range for the current batch
    start_row <- (i - 1) * batch_size + 1
    end_row <- min(i * batch_size, total_rows)
    
    # Record the start time for the current batch
    batch_start_time <- Sys.time()
    
    # Process the current batch
    current_batch <- data[start_row:end_row, ]
    
    data$stemmed_word[start_row:end_row] <- sapply(current_batch$word, function(w) {
      # Apply the write_tokens function
      stemmed <- write_tokens(w)  # Apply your write_tokens function
      # Extract the second element from the token result
      stemmed <- sapply(strsplit(stemmed, "\t"), `[`, 2)
      stemmed
    })
    
    # Record the end time for the current batch
    batch_end_time <- Sys.time()
    
    # Calculate time taken for the current batch in seconds
    batch_time <- as.numeric(difftime(batch_end_time, batch_start_time, units = "secs"))
    batch_times[i] <- batch_time
    
    # Calculate elapsed time
    elapsed_time <- as.numeric(difftime(batch_end_time, start_time, units = "secs"))
    
    # Estimate total time based on average time per batch
    average_time_per_batch <- mean(batch_times[1:i])
    estimated_total_time <- average_time_per_batch * num_batches
    estimated_remaining_time <- estimated_total_time - elapsed_time
    
    # Format time for readability
    format_time <- function(seconds) {
      sprintf("%02dh:%02dm:%02ds",
              floor(seconds / 3600),
              floor((seconds %% 3600) / 60),
              floor(seconds %% 60))
    }
    
    # Print progress with time information
    cat(sprintf("Processed batch %d/%d (Rows %d to %d) | Batch Time: %s | Elapsed Time: %s | Estimated Remaining Time: %s\n",
                i, num_batches, start_row, end_row,
                format_time(batch_time),
                format_time(elapsed_time),
                format_time(max(0, estimated_remaining_time))))
  }
  
  # Calculate total processing time
  total_processing_time <- Sys.time() - start_time
  cat(sprintf("Completed processing %d rows in %s.\n",
              total_rows,
              format_time(as.numeric(total_processing_time, units = "secs"))))
  
  return(data)
}

# Apply the batch processing function to tidy_text
batch_size <- 1000  # Adjust the batch size as needed
tidy_text_ <- process_in_batches(n_tokenTidy, batch_size)





tidy_CSX <- process_in_batches(Crosentilex_sve, batch_size)
```

```{r}
tidy_text_ %>%
  group_by(stemmed_word) %>%
  summarise(count = n()) %>%
  mutate(percent = round(count / sum(count) * 100,2)) %>% 
  arrange(desc(count)) %>%
  filter(count > 10) %>%
  datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
```
##### Doprinos riječi sentimentu (crosentilex)

```{r}
doprinos_sentimentu <- function(dataset, no = n) {
dataset %>%
  inner_join(CroSentilex_Gold, by = "word") %>% 
  count(word, sentiment,sort = TRUE) %>% 
  group_by(sentiment) %>%
  top_n(no) %>%
  ungroup() %>%
  mutate(sentiment = case_when(sentiment == 0 ~ "NEUTRALNO",
                                 sentiment == 1 ~ "NEGATIVNO",
                                 sentiment == 2 ~ "POZITIVNO")) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  ggtitle( "Sentiment") +
  labs( x = "Riječ", y = "Number of words") +
  facet_wrap(~ sentiment, scales = "free_y") +
  coord_flip() +
  scale_fill_manual(values = c("grey40", "grey50","grey60")) +  # Assuming two sentiment values; adjust as needed
  theme_minimal() + 
  theme(
    panel.background = element_blank(),
    strip.background = element_blank(),
    panel.grid = element_blank()
  ) -> gg_doprinos_sentimentu
  
 gg_doprinos_sentimentu
 
}
doprinos_sentimentu(n_tokenTidy,30)

```


##### Doprinos riječi sentimentu (NRC)

```{r echo=F, eval=T, message=F , warning= FALSE}
NRCpn <- LilaHR_long %>% rename("word" = "rijec") %>%
  filter(Emotion %in% c("Positive","Negative")) %>%
  mutate(Emotion = recode(Emotion,
                          "Positive" = "Pozitivno",
                          "Negative" = "Negativno"))


## Sentiment 
doprinos_sentimentu <- function(dataset, no = n) {
dataset %>%
  inner_join(NRCpn, by = "word") %>% 
  count(word, Emotion,sort = TRUE) %>% 
  group_by(Emotion) %>%
  top_n(no) %>%
  ungroup() %>%
#  mutate(sentiment = case_when(sentiment == 0 ~ "NEUTRAL",
#                                 sentiment == 1 ~ "NEGATIVE",
#                                 sentiment == 2 ~ "POSITIVE")) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = Emotion)) +
  geom_col(show.legend = FALSE) +
  ggtitle( "Sentiment") +
  labs( x = "Riječ", y = "Broj riječi") +
  facet_wrap(~ Emotion, scales = "free_y") +
  coord_flip() +
  scale_fill_manual(values = c("grey40", "grey50")) +  # Assuming two sentiment values; adjust as needed
  theme_minimal() + 
  theme(
    panel.background = element_blank(),
    strip.background = element_blank(),
    panel.grid = element_blank()
  ) -> gg_doprinos_sentimentu
  
 gg_doprinos_sentimentu
 
}




doprinos_sentimentu(n_tokenTidy,30)

```
##### Doprinos riječi raznom sentimentu (NRC)
```{r echo=F, eval=T, message=F , warning= FALSE, fig.width=16, fig.height=25}


NRC <- LilaHR_long %>% rename("word" = "rijec") %>%
  filter(Emotion %in% c("Anger","Anticipation","Disgust","Fear","Joy","Sadness","Surprise","Trust")) %>%
  mutate(Emotion = recode(Emotion,
                          "Anger" = "Ljutnja",
                          "Anticipation" = "Iščekivanje",
                          "Disgust" = "Gađenje",
                          "Fear" = "Strah",
                          "Joy" = "Zadovoljstvo",
                          "Sadness" = "Tuga",
                          "Surprise" = "Iznenađenje",
                          "Trust" = "Povjerenje"))


## Sentiment 
doprinos_sentimentu_full <- function(dataset, no = n) {
dataset %>%
  inner_join(NRC, by = "word") %>% 
  count(word, Emotion,sort = TRUE) %>% 
  group_by(Emotion,) %>%
  top_n(no) %>%
  ungroup() %>%
#  mutate(sentiment = case_when(sentiment == 0 ~ "NEUTRAL",
#                                 sentiment == 1 ~ "NEGATIVE",
#                                 sentiment == 2 ~ "POSITIVE")) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = Emotion)) +
  geom_col(show.legend = FALSE) +
  ggtitle( "Sentiment") +
  labs( x = "Riječ", y = "Broj riječi") +
  facet_wrap(~ Emotion, scales = "free_y") +
  coord_flip() +
  scale_fill_manual(values = c("grey10", "grey20","grey30","grey40","grey50","grey60","grey70","grey80")) +  # Assuming two sentiment values; adjust as needed
  theme_minimal() + 
  theme(
    panel.background = element_blank(),
    strip.background = element_blank(),
    panel.grid = element_blank()
  ) -> gg_doprinos_sentimentu
  
 gg_doprinos_sentimentu
 
}
doprinos_sentimentu_full(n_tokenTidy,20)
```


#### Oblak riječi sa sentimentom CroSentilex

```{r echo=F, eval=T, message=F , warning= FALSE}
## ComparisonCloud
n_tokenTidy %>%
  inner_join(CroSentilex_Gold,by="word") %>% 
  count(word, sentiment) %>% 
  top_n(200) %>%
  mutate(sentiment = case_when(sentiment == 0 ~ "+/-",
                                 sentiment == 1 ~ "-",
                                 sentiment == 2 ~ "+")) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("firebrick3", "deepskyblue3","darkslategray"),
                   max.words = 120)

```

#### Oblak riječi sa sentimentom NRC


```{r echo=F, eval=T, message=F , warning= FALSE}
n_tokenTidy %>%
  inner_join(NRCpn,by="word") %>% 
  count(word, Emotion) %>% 
  top_n(200) %>%
#  mutate(sentiment = case_when(sentiment == 0 ~ "+/-",
#                                sentiment == 1 ~ "-",
#                                 sentiment == 2 ~ "+")) %>%
  acast(word ~ Emotion, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("firebrick3", "deepskyblue3","darkslategray"),
                   max.words = 120)
```
```{r eval = T, echo = F, message=F, warning=F, fig.height=15, fig.width=15}
fb_bigram <- df %>%
  unnest_tokens(bigram, TITLE, token = "ngrams", n = 2)
#fb_bigram %>% head(10)
# fb_bigram %>%
#   count(bigram, sort = T) %>%
#   head(25) 
fb_bigram_sep <- fb_bigram %>%
  separate(bigram, c("word1","word2"), sep = " ")
fb_bigram_tidy <- fb_bigram_sep %>%
  filter(!word1 %in% stop_corpus$word) %>%
  filter(!word2 %in% stop_corpus$word) %>%
  mutate(word1 = gsub("\\d+", NA, word1)) %>%
  mutate(word2 = gsub("\\d+", NA, word2)) %>%
  mutate(word1 = gsub("^[a-zA-Z]$", NA, word1)) %>%
  mutate(word2 = gsub("^[a-zA-Z]$", NA, word2)) 
fb_bigram_tidy_bigram_counts <- fb_bigram_tidy %>% 
  count(word1, word2, sort = TRUE)

bigrams_united <- fb_bigram_tidy %>%
  unite(bigram, word1, word2, sep = " ") %>%
  filter(., !grepl("NA",bigram))

bigrams_united %>%
  count(bigram, sort = T) %>%
  filter(n>1) %>%
  datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
```


#### Najčešće fraze (bigrami; moguće i za 3 ili više riječi)(txt)

```{r eval = T, echo = F, message=F, warning=F, fig.height=15, fig.width=15}
fb_bigram <- df %>%
  unnest_tokens(bigram, FULL_TEXT, token = "ngrams", n = 2)
#fb_bigram %>% head(10)
# fb_bigram %>%
#   count(bigram, sort = T) %>%
#   head(25) 
fb_bigram_sep <- fb_bigram %>%
  separate(bigram, c("word1","word2"), sep = " ")
fb_bigram_tidy <- fb_bigram_sep %>%
  filter(!word1 %in% stop_corpus$word) %>%
  filter(!word2 %in% stop_corpus$word) %>%
  mutate(word1 = gsub("\\d+", NA, word1)) %>%
  mutate(word2 = gsub("\\d+", NA, word2)) %>%
  mutate(word1 = gsub("^[a-zA-Z]$", NA, word1)) %>%
  mutate(word2 = gsub("^[a-zA-Z]$", NA, word2)) 
fb_bigram_tidy_bigram_counts <- fb_bigram_tidy %>% 
  count(word1, word2, sort = TRUE)

bigrams_united <- fb_bigram_tidy %>%
  unite(bigram, word1, word2, sep = " ") %>%
  filter(., !grepl("NA",bigram))
#bigrams_united
#bigrams_united %>% 
#  count(FROM,bigram,sort = T) -> topicBigram

bigrams_united %>%
  count(bigram, sort = T) %>%
  filter(n>1) %>%
  datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
```


#### TEMATSKA ANLIZA (2 teme)
```{r eval = T, echo = F, message=F, warning=F, fig.height=15, fig.width=15}

dtm <- n_tokenTidy %>%
  count(URL, word) %>%
  cast_dtm(URL, word, n)

# Perform Latent Dirichlet Allocation (LDA) for topic modeling
# Here, k = number of topics. You can adjust this based on your data
lda_model2 <- LDA(dtm, k = 2, control = list(seed = 1234))

# Tidy the LDA output
lda_topics <- tidy(lda_model2, matrix = "beta")

# Get the top terms for each topic
top_terms <- lda_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 15) %>% # Adjust 'n' for the number of terms you want to show
  ungroup() %>%
  arrange(topic, -beta)

# Print the top terms for each topic
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "",
       x = NULL, y = "Beta")




```

#### TEMATSKA ANLIZA (3 teme)
```{r eval = T, echo = F, message=F, warning=F, fig.height=15, fig.width=15}

dtm <- n_tokenTidy %>%
  count(URL, word) %>%
  cast_dtm(URL, word, n)

# Perform Latent Dirichlet Allocation (LDA) for topic modeling
# Here, k = number of topics. You can adjust this based on your data
lda_model2 <- LDA(dtm, k = 3, control = list(seed = 1234))

# Tidy the LDA output
lda_topics <- tidy(lda_model2, matrix = "beta")

# Get the top terms for each topic
top_terms <- lda_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 15) %>% # Adjust 'n' for the number of terms you want to show
  ungroup() %>%
  arrange(topic, -beta)

# Print the top terms for each topic
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "",
       x = NULL, y = "Beta")




```
# Text analysis (TEXT)


```{r echo=F, eval=T, message=F , warning= FALSE}
# dim before tokenize
#dim(dta)

# tokenize
df %>% 
  unnest_tokens(word, FULL_TEXT) -> n_token

# dim after tokenize
#dim(n_token)

# check
# fb_token %>% 
#   select(FROM, word, MENTION_SNIPPET ) %>%
#     sample_n(.,100)

# remove stop words, numbers, single letters
n_token %>% 
  anti_join(stop_corpus, by = "word") %>%
  mutate(word = gsub("\\d+", NA, word)) %>%
  mutate(word = gsub("^[a-zA-Z]$", NA, word)) -> n_tokenTidy
# remove NA
n_tokenTidy %>%
  filter(!is.na(word)) -> n_tokenTidy

# check
# fb_tokenTidy  %>% 
#   select(FROM, word, MENTION_SNIPPET ) %>%
#   sample_n(.,100)

# dim after clean
#dim(n_tokenTidy)

```


## STEMMING

```{r}
# Apply stemming to the "word" column
tidy_text_ <- n_tokenTidy %>%
  mutate(
    stemmed_word = sapply(word, function(w) {
      stemmed = write_tokens(w)  # Apply your write_tokens function
      # Process the result similar to your generalno example
      stemmed = sapply(strsplit(stemmed, "\t"), `[`, 2)  # Extract the second element
      stemmed
    })
  )

# Enframe the results if necessary
tidy_text_ <- tidy_text_ %>%
  rename(original_word = word) %>%
  select(URL, original_word, stemmed_word)
```


```{r}
process_in_batches <- function(data, batch_size = 1000) {
  total_rows <- nrow(data)
  num_batches <- ceiling(total_rows / batch_size)
  
  # Initialize a new column to store stemmed words
  data$stemmed_word <- NA
  
  # Initialize variables to track time
  start_time <- Sys.time()
  batch_times <- numeric(num_batches)  # To store time taken for each batch
  
  for (i in seq_len(num_batches)) {
    # Calculate the range for the current batch
    start_row <- (i - 1) * batch_size + 1
    end_row <- min(i * batch_size, total_rows)
    
    # Record the start time for the current batch
    batch_start_time <- Sys.time()
    
    # Process the current batch
    current_batch <- data[start_row:end_row, ]
    
    data$stemmed_word[start_row:end_row] <- sapply(current_batch$word, function(w) {
      # Apply the write_tokens function
      stemmed <- write_tokens(w)  # Apply your write_tokens function
      # Extract the second element from the token result
      stemmed <- sapply(strsplit(stemmed, "\t"), `[`, 2)
      stemmed
    })
    
    # Record the end time for the current batch
    batch_end_time <- Sys.time()
    
    # Calculate time taken for the current batch in seconds
    batch_time <- as.numeric(difftime(batch_end_time, batch_start_time, units = "secs"))
    batch_times[i] <- batch_time
    
    # Calculate elapsed time
    elapsed_time <- as.numeric(difftime(batch_end_time, start_time, units = "secs"))
    
    # Estimate total time based on average time per batch
    average_time_per_batch <- mean(batch_times[1:i])
    estimated_total_time <- average_time_per_batch * num_batches
    estimated_remaining_time <- estimated_total_time - elapsed_time
    
    # Format time for readability
    format_time <- function(seconds) {
      sprintf("%02dh:%02dm:%02ds",
              floor(seconds / 3600),
              floor((seconds %% 3600) / 60),
              floor(seconds %% 60))
    }
    
    # Print progress with time information
    cat(sprintf("Processed batch %d/%d (Rows %d to %d) | Batch Time: %s | Elapsed Time: %s | Estimated Remaining Time: %s\n",
                i, num_batches, start_row, end_row,
                format_time(batch_time),
                format_time(elapsed_time),
                format_time(max(0, estimated_remaining_time))))
  }
  
  # Calculate total processing time
  total_processing_time <- Sys.time() - start_time
  cat(sprintf("Completed processing %d rows in %s.\n",
              total_rows,
              format_time(as.numeric(total_processing_time, units = "secs"))))
  
  return(data)
}

# Apply the batch processing function to tidy_text
batch_size <- 1000  # Adjust the batch size as needed
tidy_text_ <- process_in_batches(n_tokenTidy, batch_size)





tidy_CSX <- process_in_batches(Crosentilex_sve, batch_size)
```

```{r}
tidy_text_ %>%
  group_by(stemmed_word) %>%
  summarise(count = n()) %>%
  mutate(percent = round(count / sum(count) * 100,2)) %>% 
  arrange(desc(count)) %>%
  filter(count > 50) %>%
  datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
```
##### Doprinos riječi sentimentu (crosentilex)

```{r}
doprinos_sentimentu <- function(dataset, no = n) {
dataset %>%
  inner_join(CroSentilex_Gold, by = "word") %>% 
  count(word, sentiment,sort = TRUE) %>% 
  group_by(sentiment) %>%
  top_n(no) %>%
  ungroup() %>%
  mutate(sentiment = case_when(sentiment == 0 ~ "NEUTRALNO",
                                 sentiment == 1 ~ "NEGATIVNO",
                                 sentiment == 2 ~ "POZITIVNO")) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  ggtitle( "Sentiment") +
  labs( x = "Riječ", y = "Number of words") +
  facet_wrap(~ sentiment, scales = "free_y") +
  coord_flip() +
  scale_fill_manual(values = c("grey40", "grey50","grey60")) +  # Assuming two sentiment values; adjust as needed
  theme_minimal() + 
  theme(
    panel.background = element_blank(),
    strip.background = element_blank(),
    panel.grid = element_blank()
  ) -> gg_doprinos_sentimentu
  
 gg_doprinos_sentimentu
 
}
doprinos_sentimentu(n_tokenTidy,30)

```


##### Doprinos riječi sentimentu (NRC)

```{r echo=F, eval=T, message=F , warning= FALSE}
NRCpn <- LilaHR_long %>% rename("word" = "rijec") %>%
  filter(Emotion %in% c("Positive","Negative")) %>%
  mutate(Emotion = recode(Emotion,
                          "Positive" = "Pozitivno",
                          "Negative" = "Negativno"))


## Sentiment 
doprinos_sentimentu <- function(dataset, no = n) {
dataset %>%
  inner_join(NRCpn, by = "word") %>% 
  count(word, Emotion,sort = TRUE) %>% 
  group_by(Emotion) %>%
  top_n(no) %>%
  ungroup() %>%
#  mutate(sentiment = case_when(sentiment == 0 ~ "NEUTRAL",
#                                 sentiment == 1 ~ "NEGATIVE",
#                                 sentiment == 2 ~ "POSITIVE")) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = Emotion)) +
  geom_col(show.legend = FALSE) +
  ggtitle( "Sentiment") +
  labs( x = "Riječ", y = "Broj riječi") +
  facet_wrap(~ Emotion, scales = "free_y") +
  coord_flip() +
  scale_fill_manual(values = c("grey40", "grey50")) +  # Assuming two sentiment values; adjust as needed
  theme_minimal() + 
  theme(
    panel.background = element_blank(),
    strip.background = element_blank(),
    panel.grid = element_blank()
  ) -> gg_doprinos_sentimentu
  
 gg_doprinos_sentimentu
 
}




doprinos_sentimentu(n_tokenTidy,30)

```
##### Doprinos riječi raznom sentimentu (NRC)
```{r echo=F, eval=T, message=F , warning= FALSE, fig.width=16, fig.height=25}


NRC <- LilaHR_long %>% rename("word" = "rijec") %>%
  filter(Emotion %in% c("Anger","Anticipation","Disgust","Fear","Joy","Sadness","Surprise","Trust")) %>%
  mutate(Emotion = recode(Emotion,
                          "Anger" = "Ljutnja",
                          "Anticipation" = "Iščekivanje",
                          "Disgust" = "Gađenje",
                          "Fear" = "Strah",
                          "Joy" = "Zadovoljstvo",
                          "Sadness" = "Tuga",
                          "Surprise" = "Iznenađenje",
                          "Trust" = "Povjerenje"))


## Sentiment 
doprinos_sentimentu_full <- function(dataset, no = n) {
dataset %>%
  inner_join(NRC, by = "word") %>% 
  count(word, Emotion,sort = TRUE) %>% 
  group_by(Emotion,) %>%
  top_n(no) %>%
  ungroup() %>%
#  mutate(sentiment = case_when(sentiment == 0 ~ "NEUTRAL",
#                                 sentiment == 1 ~ "NEGATIVE",
#                                 sentiment == 2 ~ "POSITIVE")) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = Emotion)) +
  geom_col(show.legend = FALSE) +
  ggtitle( "Sentiment") +
  labs( x = "Riječ", y = "Broj riječi") +
  facet_wrap(~ Emotion, scales = "free_y") +
  coord_flip() +
  scale_fill_manual(values = c("grey10", "grey20","grey30","grey40","grey50","grey60","grey70","grey80")) +  # Assuming two sentiment values; adjust as needed
  theme_minimal() + 
  theme(
    panel.background = element_blank(),
    strip.background = element_blank(),
    panel.grid = element_blank()
  ) -> gg_doprinos_sentimentu
  
 gg_doprinos_sentimentu
 
}
doprinos_sentimentu_full(n_tokenTidy,20)
```


#### Oblak riječi sa sentimentom CroSentilex

```{r echo=F, eval=T, message=F , warning= FALSE}
## ComparisonCloud
n_tokenTidy %>%
  inner_join(CroSentilex_Gold,by="word") %>% 
  count(word, sentiment) %>% 
  top_n(200) %>%
  mutate(sentiment = case_when(sentiment == 0 ~ "+/-",
                                 sentiment == 1 ~ "-",
                                 sentiment == 2 ~ "+")) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("firebrick3", "deepskyblue3","darkslategray"),
                   max.words = 120)

```

#### Oblak riječi sa sentimentom NRC


```{r echo=F, eval=T, message=F , warning= FALSE}
n_tokenTidy %>%
  inner_join(NRCpn,by="word") %>% 
  count(word, Emotion) %>% 
  top_n(200) %>%
#  mutate(sentiment = case_when(sentiment == 0 ~ "+/-",
#                                sentiment == 1 ~ "-",
#                                 sentiment == 2 ~ "+")) %>%
  acast(word ~ Emotion, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("firebrick3", "deepskyblue3","darkslategray"),
                   max.words = 120)
```



```{r eval = T, echo = F, message=F, warning=F, fig.height=15, fig.width=15}
fb_bigram <- df %>%
  unnest_tokens(bigram, TITLE, token = "ngrams", n = 2)
#fb_bigram %>% head(10)
# fb_bigram %>%
#   count(bigram, sort = T) %>%
#   head(25) 
fb_bigram_sep <- fb_bigram %>%
  separate(bigram, c("word1","word2"), sep = " ")
fb_bigram_tidy <- fb_bigram_sep %>%
  filter(!word1 %in% stop_corpus$word) %>%
  filter(!word2 %in% stop_corpus$word) %>%
  mutate(word1 = gsub("\\d+", NA, word1)) %>%
  mutate(word2 = gsub("\\d+", NA, word2)) %>%
  mutate(word1 = gsub("^[a-zA-Z]$", NA, word1)) %>%
  mutate(word2 = gsub("^[a-zA-Z]$", NA, word2)) 
fb_bigram_tidy_bigram_counts <- fb_bigram_tidy %>% 
  count(word1, word2, sort = TRUE)

bigrams_united <- fb_bigram_tidy %>%
  unite(bigram, word1, word2, sep = " ") %>%
  filter(., !grepl("NA",bigram))

bigrams_united %>%
  count(bigram, sort = T) %>%
  filter(n>1) %>%
  datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
```


#### Najčešće fraze (bigrami; moguće i za 3 ili više riječi)(txt)

```{r eval = T, echo = F, message=F, warning=F, fig.height=15, fig.width=15}
fb_bigram <- df %>%
  unnest_tokens(bigram, FULL_TEXT, token = "ngrams", n = 2)
#fb_bigram %>% head(10)
# fb_bigram %>%
#   count(bigram, sort = T) %>%
#   head(25) 
fb_bigram_sep <- fb_bigram %>%
  separate(bigram, c("word1","word2"), sep = " ")
fb_bigram_tidy <- fb_bigram_sep %>%
  filter(!word1 %in% stop_corpus$word) %>%
  filter(!word2 %in% stop_corpus$word) %>%
  mutate(word1 = gsub("\\d+", NA, word1)) %>%
  mutate(word2 = gsub("\\d+", NA, word2)) %>%
  mutate(word1 = gsub("^[a-zA-Z]$", NA, word1)) %>%
  mutate(word2 = gsub("^[a-zA-Z]$", NA, word2)) 
fb_bigram_tidy_bigram_counts <- fb_bigram_tidy %>% 
  count(word1, word2, sort = TRUE)

bigrams_united <- fb_bigram_tidy %>%
  unite(bigram, word1, word2, sep = " ") %>%
  filter(., !grepl("NA",bigram))
#bigrams_united
#bigrams_united %>% 
#  count(FROM,bigram,sort = T) -> topicBigram

bigrams_united %>%
  count(bigram, sort = T) %>%
  filter(n>1) %>%
  datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
```


#### TEMATSKA ANLIZA (2 teme)
```{r eval = T, echo = F, message=F, warning=F, fig.height=15, fig.width=15}

dtm <- n_tokenTidy %>%
  count(URL, word) %>%
  cast_dtm(URL, word, n)

# Perform Latent Dirichlet Allocation (LDA) for topic modeling
# Here, k = number of topics. You can adjust this based on your data
lda_model2 <- LDA(dtm, k = 2, control = list(seed = 1234))

# Tidy the LDA output
lda_topics <- tidy(lda_model2, matrix = "beta")

# Get the top terms for each topic
top_terms <- lda_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 15) %>% # Adjust 'n' for the number of terms you want to show
  ungroup() %>%
  arrange(topic, -beta)

# Print the top terms for each topic
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "",
       x = NULL, y = "Beta")




```

#### TEMATSKA ANLIZA (3 teme)
```{r eval = T, echo = F, message=F, warning=F, fig.height=15, fig.width=15}

dtm <- n_tokenTidy %>%
  count(URL, word) %>%
  cast_dtm(URL, word, n)

# Perform Latent Dirichlet Allocation (LDA) for topic modeling
# Here, k = number of topics. You can adjust this based on your data
lda_model2 <- LDA(dtm, k = 3, control = list(seed = 1234))

# Tidy the LDA output
lda_topics <- tidy(lda_model2, matrix = "beta")

# Get the top terms for each topic
top_terms <- lda_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 15) %>% # Adjust 'n' for the number of terms you want to show
  ungroup() %>%
  arrange(topic, -beta)

# Print the top terms for each topic
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "",
       x = NULL, y = "Beta")




```


### 1) OKVIR × MEDIJ


```{r echo=FALSE, eval=TRUE}

df %>% 
  count(MEDIJ, OKVIR) %>% 
  pivot_wider(names_from = OKVIR, values_from = n, values_fill = 0) 




```

```{r echo=FALSE, eval=TRUE}
df %>%
  count(MEDIJ, OKVIR) %>%
  group_by(MEDIJ) %>%
  mutate(prop = n / sum(n)) %>%
  ungroup() %>%
  ggplot(aes(x = fct_reorder(MEDIJ, -prop), y = prop, fill = OKVIR)) +
  geom_col(position = "stack") +
  coord_flip() +
  labs(
    x = "MEDIJ",
    y = "Udio članaka",
    fill = "OKVIR",
    title = "Proporcija OKVIRa po medijima"
  ) +
  theme_minimal()
```

```{r echo=FALSE, eval=TRUE}
tbl <- table(df$MEDIJ, df$OKVIR)
chisq_res <- chisq.test(tbl)
chisq_res
```

```{r}
# show 20 most frequent words per OKVIR from this object tidy_text_

tidy_text_ %>%
  group_by(MEDIJ, word) %>%
  summarise(n = n(), .groups = 'drop') %>%
  arrange(MEDIJ, desc(n)) %>%
  group_by(MEDIJ) %>%
  slice_head(n = 10) %>%
  ungroup() %>%
  ggplot(aes(x = reorder_within(word, n, MEDIJ), y = n, fill = MEDIJ)) +
  geom_col(show.legend = FALSE) +
  # This helper function cleans up the axis labels created by reorder_within
  scale_x_reordered() + 
  # Use scales = "free" to allow both axes to vary between facets
  facet_wrap(~ MEDIJ, scales = "free") +
  coord_flip() +
  labs(
    # Note: After coord_flip(), the x-axis label applies to the vertical axis
    x = "Word",
    y = "Count",
    title = "Top 10 Words per MEDIJ"
  ) +
  theme_minimal()




```


```{r}
tidy_text_ %>%
  group_by(OKVIR, word) %>%
  summarise(n = n(), .groups = 'drop') %>%
  arrange(OKVIR, desc(n)) %>%
  group_by(OKVIR) %>%
  slice_head(n = 10) %>%
  ungroup() %>%
  ggplot(aes(x = reorder_within(word, n, OKVIR), y = n, fill = OKVIR)) +
  geom_col(show.legend = FALSE) +
  # This helper function cleans up the axis labels created by reorder_within
  scale_x_reordered() + 
  # Use scales = "free" to allow both axes to vary between facets
  facet_wrap(~ OKVIR, scales = "free") +
  coord_flip() +
  labs(
    # Note: After coord_flip(), the x-axis label applies to the vertical axis
    x = "Word",
    y = "Count",
    title = "Top 10 Words per OKVIR"
  ) +
  theme_minimal()
```



### 2) OKVIR x “vrijeme objave” (MONTH)


```{r okvir-vs-hour-summary, echo=FALSE, eval=T}
df %>%
  group_by(OKVIR,MONTH) %>%
  summarize(
    n=n(),
    
  ) 





```


```{r okvir-vs-hour-plot, echo=FALSE, eval=F, fig.width=7, fig.height=4}
# 2b) Boxplot: HOUR distribution by OKVIR
ggplot(df, aes(x = OKVIR, y = MONTH)) +
  geom_boxplot() +
  labs(
    x = "OKVIR",
    y = "Publication MONTH",
    title = "Distribution of publication MONTH by OKVIR"
  ) +
  theme_minimal()

```


```{r okvir-vs-hour-test, echo=FALSE, eval=F}
# 2c) Kruskal-Wallis test: Is HOUR different across OKVIR categories?
kruskal.test(MONTH ~ OKVIR, data = df)


```


### 3) OKVIR x Izvor




```{r okvir-vs-izvori-summary, echo=FALSE, eval=TRUE}
# 3a) Summary statistics of IZVORI by OKVIR
df %>%
  group_by(OKVIR) %>%
  summarize(
    #calculate mode for IZVORI
    mode_izvori = as.character(names(sort(table(IZVORI), decreasing = TRUE)[1])),
    
  ) 

```



```{r okvir-vs-izvori-plot, echo=FALSE, eval=TRUE, fig.width=7, fig.height=4}
# 3b) Boxplot: IZVORI distribution by OKVIR
ggplot(df, aes(x = OKVIR, y = IZVORI)) +
  geom_boxplot(outlier.size = 1) +
  labs(
    x = "OKVIR",
    y = "Broj izvora (IZVORI)",
    title = "Distrinucija IZVORI x OKVIR"
  ) +
  theme_minimal()


```

```{r okvir-vs-izvori-test , echo=FALSE, eval=TRUE}
# 3c) Kruskal-Wallis: IZVORI ~ OKVIR
kruskal.test(IZVORI ~ OKVIR, data = df)

```


```{r}
tidy_text_ %>%
  group_by(IZVORI, word) %>%
  summarise(n = n(), .groups = 'drop') %>%
  arrange(IZVORI, desc(n)) %>%
  group_by(IZVORI) %>%
  slice_head(n = 10) %>%
  ungroup() %>%
  ggplot(aes(x = reorder_within(word, n, IZVORI), y = n, fill = IZVORI)) +
  geom_col(show.legend = FALSE) +
  # This helper function cleans up the axis labels created by reorder_within
  scale_x_reordered() + 
  # Use scales = "free" to allow both axes to vary between facets
  facet_wrap(~ IZVORI, scales = "free") +
  coord_flip() +
  labs(
    # Note: After coord_flip(), the x-axis label applies to the vertical axis
    x = "Word",
    y = "Count",
    title = "Top 10 Words per IZVORI"
  ) +
  theme_minimal()
```







### 4) OKVIR x Sentiment

```{r okvir-vs-sentiment-summary , echo=FALSE, eval=TRUE}
# 4a) Count of SENTIMENT × OKVIR
df %>%
  count(OKVIR, SENTIMENT) %>%
  pivot_wider(names_from = SENTIMENT, values_from = n, values_fill = 0) 
```

```{r okvir-vs-sentiment-plot, fig.width=7, fig.height=5, echo=FALSE, eval=TRUE}
# 4b) Proportional bar chart: SENTIMENT within each OKVIR
df %>%
  count(OKVIR, SENTIMENT) %>%
  group_by(OKVIR) %>%
  mutate(prop = n / sum(n)) %>%
  ungroup() %>%
  ggplot(aes(x = OKVIR, y = prop, fill = factor(SENTIMENT))) +
  geom_col(position = "stack") +
  labs(
    x = "OKVIR",
    y = "Proportion",
    fill = "SENTIMENT",
    title = "Distribution of sentiment levels across OKVIR"
  ) +
  theme_minimal()


```





```{r okvir-vs-sentiment-test , echo=FALSE, eval=TRUE}
# 4c) If SENTIMENT is treated numerically, compare medians by OKVIR:
kruskal.test(SENTIMENT ~ OKVIR, data = df)

```

### 5) OKVIR x Doseg (reach)



```{r okvir-vs-doseg-summary, echo=FALSE, eval=TRUE}
# 5a) Summary stats of DOSEG by OKVIR
df %>%
  group_by(OKVIR) %>%
  summarize(
    n = n(),
    mean_reach = mean(DOSEG, na.rm = TRUE),
    median_reach = median(DOSEG, na.rm = TRUE),
    IQR_reach = IQR(DOSEG, na.rm = TRUE)
  ) 


```



```{r okvir-vs-doseg-plot, fig.width=7, fig.height=4, echo=FALSE, eval=TRUE}
# 5b) Boxplot (plus jitter) of DOSEG by OKVIR
ggplot(df, aes(x = OKVIR, y = DOSEG)) +
  geom_boxplot(outlier.shape = NA, alpha = 0.6) +
  geom_jitter(width = 0.2, alpha = 0.3, size = 0.7) +
  labs(
    x = "OKVIR",
    y = "Reach (DOSEG)",
    title = "DOSEG distribution across OKVIR categories"
  ) +
  theme_minimal()
```

```{r okvir-vs-doseg-test,echo=FALSE, eval=TRUE}
# 5c) Kruskal-Wallis test for DOSEG ~ OKVIR
kruskal.test(DOSEG ~ OKVIR, data = df)
```



```{r}
tidy_text_ %>%
  group_by(DOSEG, word) %>%
  summarise(n = n(), .groups = 'drop') %>%
  arrange(DOSEG, desc(n)) %>%
  group_by(DOSEG) %>%
  slice_head(n = 10) %>%
  ungroup() %>%
  ggplot(aes(x = reorder_within(word, n, DOSEG), y = n, fill = DOSEG)) +
  geom_col(show.legend = FALSE) +
  # This helper function cleans up the axis labels created by reorder_within
  scale_x_reordered() + 
  # Use scales = "free" to allow both axes to vary between facets
  facet_wrap(~ DOSEG, scales = "free") +
  coord_flip() +
  labs(
    # Note: After coord_flip(), the x-axis label applies to the vertical axis
    x = "Word",
    y = "Count",
    title = "Top 10 Words per DOSEG"
  ) +
  theme_minimal()
```




### 6) OKVIR x Komentari (comments)

```{r okvir-vs-komentari-summary,echo=FALSE, eval=TRUE}
# 6a) Summary stats of KOMENTARI by OKVIR
df %>%
  group_by(OKVIR) %>%
  summarize(
    n = n(),
    mean_comments = mean(KOMENTARI, na.rm = TRUE),
    median_comments = median(KOMENTARI, na.rm = TRUE),
    IQR_comments = IQR(KOMENTARI, na.rm = TRUE)
  ) 

```




```{r okvir-vs-komentari-plot, fig.width=7, fig.height=4,echo=FALSE, eval=TRUE}
# 6b) Boxplot (plus jitter) of KOMENTARI by OKVIR
ggplot(df, aes(x = OKVIR, y = KOMENTARI)) +
  geom_boxplot(outlier.shape = NA, alpha = 0.6) +
  geom_jitter(width = 0.2, alpha = 0.3, size = 0.7, color = "steelblue") +
  labs(
    x = "OKVIR",
    y = "Number of comments (KOMENTARI)",
    title = "KOMENTARI distribution across OKVIR categories"
  ) +
  theme_minimal()
```

```{r okvir-vs-komentari-test,echo=FALSE, eval=TRUE}
# 6c) Kruskal-Wallis test: KOMENTARI ~ OKVIR
kruskal.test(KOMENTARI ~ OKVIR, data = df)
```





















